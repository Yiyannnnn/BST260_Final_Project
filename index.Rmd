---
title: "Machine Learning Application on Human Gut Microbiome Data"
author: "Yiyan Zhang"
output: html_document
---

### Introduction

The human body (especially the gastrointestinal tract) harbors a diverse of microorganisms that includes bacteria, protozoa, archaea, viruses, and fungi (Huseyin et al., 2017). The gut microbiota has notable influence on the host health and the dysbiosis of gut microbiota has been associated with many diseases (Thursby & Juge, 2017). Thus, in this project, I would take the advantage of the metagenomic information from human gut to make the classification between healthy and disease samples with several popular machine learning models. 

In this study, I leveraged gut microbiome samples from a publicly available large-scale metagenomic database --- curatedMetagenomicData (CMD; version 3.6.0) which contains uniformly processed human microbiome taxonomic abundances data and phenotypic data (Pasolli et al., 2017). I selected samples based on two criteria: the host age is between 18 and 65; no antibiotic use. Samples include healthy individuals, as well individuals with one of the following diseases: Inflammatory Bowel Disease (IBD), Colorectal cancer (CRC), Impaired Glucose Tolerance (IGT), and Type 2 Diabetes (T2D). The final dataset includes 2,815 healthy samples, 768 IBD samples, 368 CRC samples, 199 IGT samples, and 164 T2D samples. I also filtered out those species whose prevalence were lower than 0.1, resulting in 194 species. Here, we performed explanatory data analysis to get the overall view of the structure of our data. For age, we have a relatively elder cohort for CRC and IGT, which are around 55 years old. For gender, the overall proportion are relatively close, but for three of our disease cohort (CRC IGT and T2D) is dominant by male. Luckily, we do not have much missingness in our data. Also, I explored alpha diversity of each disease status for microbiome composition as a regular microbiome data explanatory approach. It is measuring the diversity of microbiome species within an individual sample. The oldest and the simplest concept of species diversity: the total number of species present in a sample or community, which is the observed here. Chao 1 index is based upon the number of rare species found in a sample. Shannon diversity index is one of the most popular measures of species diversity. It considers the differences in proportion or abundance of each species. Simpson's Diversity Index is a measure of diversity which considers the number of species present, as well as the relative abundance of each species. From the violin plots for different measurements, we do not observed much difference among groups.

In this analysis, I will apply PCA between healthy and each disease group first, as the number of features is relatively large for microbiome datasets. This step can help to get sense whether disease samples are easy to differentiate from healthy samples based on microbiome composition. However, this step will loss the interpretability of the model, and one of the goals of metagenomic study is to find the taxonomy marker associated with different disease. Thus, I will use abundance table as the input for classification. I will employ the state-of-the-art machine learning methods which have been shown as powerful tools to do the classification in previous study (Wang & Liu, 2020), including logistic regression model with further feature selection using LASSO (Logit-Lasso) and Random Forest (RF). For each method, I will use k-fold cross-validation to select the best model and use k-fold cross-validation in higher structure to estimate the model performance. In order to better evaluate the model performance, I used accuracy, F1 score, and area under precision and recall curve (AUPRC). Considering we have much more heathy samples than the disease samples, I figured out F1 score and AUPRC might works better than accuracy under the imbalance data setting. I will also discuss this issue in detail at the results part.


### Results

PCA is a useful tool for analyzing and understanding the complexity of the microbiome, and it can help researchers identify important patterns and relationships that may not be immediately apparent from the raw data. First, I generated PCA plots for each disease samples comparing with healthy samples. From the plots for four diseases, we can only observe some differentiation in IBD at bottom left part. PCA plots for other diseases are Thus, we might expect the classification for most disease would be relatively hard to achieve, but IBD might have better performance among all diseases.

Then, I trained logistic regression model with variable selection using LASSO with 10-fold cross validation as we have the binary outcome of healthy or disease and a huge amount of taxonomy abundance features. I also performed 10-fold cross-validation to evaluate model performance. In the performance table, we can see that the average accuracy for IBD, CRC, IGT, and T2D are 0.918, 0.916, 0.964, and 0.946 respectively; the average F1 score are 0.806, 0.539, 0.724, and 0.347 respectively; and the average AUPRC are 0.856, 0.577, 0.572, and 0.577 respectively. I also trained random forest with 10-fold cross-validation to select best number of variables randomly sampled as candidates at each split. Similarly, I also used 10-fold cross-validation to evaluate the model performance. In the performance table, we can see that the average accuracy for IBD, CRC, IGT, and T2D are 0.954, 0.930, 0.984, and 0.951 respectively; the average F1 score are 0.883, 0.734, 0.855, and 0.319 respectively; and the average AUPRC are 0.975, 0.734, 0.975, and 0.523 respectively. All model results are presented in Table 2. Generally, as we expected, the classification of IBD is easier, but the performance for IGT classification is also relative good. Also, for T2D, we can see the performance are poor which might be caused by limited sample size. 

In addition, I investigated the important species to classify the disease status. Figure 3 shows the Venn diagram of species included for each disease logistic regression model. Noticeably, IGT included 116 species which is the largest model. In total, there are 8 species included in all models.  For random forest model, I plotted variable importance plot for models of each disease as well (see Figure 4). We found Gemmiger formicilis, Streptococcus oralis, Parabacteroides johnsonii, and Enterocloster bolteae are the model’s most important biomarker for IBD, CRC, IGT, and T2D respectively.

In order to conduct systematic comparison of model, I plotted boxplot of all model performance based on accuracy, F1 score, and AUPRC in Figure 5-7. Based on these results, it seems that the random forest model performs well across all four medical conditions, with generally higher accuracy, F1 scores, and AUPRC values compared to the logistic regression and LASSO models. However, it is worth noting that the performance of the models may vary depending on the specific characteristics of the data and the specific task at hand, and it is always important to consider multiple evaluation metrics to get a comprehensive understanding of the model's performance. The results are shown in terms of four evaluation metrics: accuracy, F1 score, and area under the precision-recall curve (AUPRC).  Accuracy is a commonly used evaluation metric in machine learning, but it may not always be the best choice, especially when the classes in the data are imbalanced like in our data. In such cases, using accuracy as the sole evaluation metric can be misleading because it does not take into account the relative sizes of the two classes. For example, if the data consists of 99% negative cases and 1% positive cases, a model that always predicts negative could achieve high accuracy simply by guessing the most common class. However, this model would not be very useful in practice because it would not be able to detect the positive cases.

In this case, using other evaluation metrics that are more sensitive to class imbalance can be more informative. One such metric is the F1 score, which is a balance of precision and recall. Precision is the proportion of true positive predictions made by the model out of all positive predictions, and recall is the proportion of true positive predictions made by the model out of all actual positive cases. The F1 score is the harmonic mean of precision and recall, and it is a good metric to use when you want to balance the false positive and false negative rates. Another metric that can be useful when the classes are imbalanced is the area under the precision-recall curve (AUPRC). The AUPRC is a measure of the model's ability to discriminate between positive and negative cases, and it is useful when the classes are imbalanced because it is more sensitive to changes in the recall rate than the precision rate.

Overall, it is important to consider multiple evaluation metrics and choose the ones that are most appropriate for the specific task and the characteristics of the data. In cases where the classes are imbalanced, using metrics such as the F1 score and the AUPRC can provide a more comprehensive understanding of the model's performance compared to using accuracy alone.


### Conclusion

In conclusion, the machine learning models applied in this study were able to effectively classify samples from healthy individuals and those with various diseases using data from the human gut microbiome. PCA was first used to get a sense of whether disease samples could be easily differentiated from healthy samples based on microbiome composition. The Logit-Lasso and RF classification methods were then applied, with k-fold cross-validation used to select the best model and estimate performance. The Logit-Lasso model performed adequately good overall, with high accuracy, F1 score, and AUPRC in all disease groups. The Random Forest model showed stronger performance and may be more suitable for cases where interpretability of the model is important. 

These results highlight the potential of machine learning techniques in analyzing and understanding the complex relationships between the gut microbiome and human health. However, this study did not cooperate much microbiome data specific features in our analysis, like sparsity, compositionality, and small n large p problem. Further studies with larger and more diverse datasets may be necessary to fully explore the potential of these methods in the field of gut microbiome research. There is also more machine learning framework are available. Also, out study are limited at two-class classification. More pivotal studies that worked on multi-class classification give me further directions on the project (Su et al., 2022).

### Reference

Huseyin, C. E., O’Toole, P. W., Cotter, P. D., & Scanlan, P. D. (2017). Forgotten fungi—The gut mycobiome in human health and disease. FEMS Microbiology Reviews, 41(4), 479–511. https://doi.org/10.1093/femsre/fuw047

Pasolli, E., Schiffer, L., Manghi, P., Renson, A., Obenchain, V., Truong, D. T., Beghini, F., Malik, F., Ramos, M., Dowd, J. B., Huttenhower, C., Morgan, M., Segata, N., & Waldron, L. (2017). Accessible, curated metagenomic data through ExperimentHub. Nature Methods, 14(11), Article 11. https://doi.org/10.1038/nmeth.4468

Thursby, E., & Juge, N. (2017). Introduction to the human gut microbiota. Biochemical Journal, 474(11), 1823–1836. https://doi.org/10.1042/BCJ20160510

Wang, X., & Liu, Y. (2020). Comparative study of classifiers for human microbiome data. Medicine in Microecology, 4, 100013. https://doi.org/10.1016/j.medmic.2020.100013

Su, Q., Liu, Q., Lau, R. I., Zhang, J., Xu, Z., Yeoh, Y. K., Leung, T. W. H., Tang, W., Zhang, L., Liang, J. Q. Y., Yau, Y. K., Zheng, J., Liu, C., Zhang, M., Cheung, C. P., Ching, J. Y. L., Tun, H. M., Yu, J., Chan, F. K. L., & Ng, S. C. (2022). Faecal microbiome-based machine learning for multi-class disease diagnosis. Nature Communications, 13(1), 6818-6818. https://doi.org/10.1038/s41467-022-34405-3